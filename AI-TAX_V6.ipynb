{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2152490d",
   "metadata": {},
   "source": [
    "<H1>Install the packages To setup the env do this only for the first time only</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd4a99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Packages\n",
    "#1. Install the files from (https://github.com/jeffheaton/app_deep_learning/blob/main/install/pytorch-install-aug-2023.ipynb)\n",
    "#1.5 Open jupyter notbook install Transformer and nltk and also write this (pip install -U pip setuptools wheel)\n",
    "#2.  After following the step write this command down on the terminal (conda install anaconda::spacy)\n",
    "#3. After that is done u are free to import the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d24e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "004875a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahe\\anaconda3\\envs\\torch\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1db3226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/6a/19e9fe04fca059ccf770861c7d5721ab4c2aebc539889e97c7977528a53b/pip-24.0-py3-none-any.whl (2.1MB)\n",
      "Collecting setuptools\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/42/be1c7bbdd83e1bfb160c94b9cafd8e25efc7400346cf7ccdbdb452c467fa/setuptools-68.0.0-py3-none-any.whl (804kB)\n",
      "Collecting wheel\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/c3/55076fc728723ef927521abaa1955213d094933dc36d4a2008d5101e1af5/wheel-0.42.0-py3-none-any.whl (65kB)\n",
      "Installing collected packages: pip, setuptools, wheel\n",
      "  Found existing installation: pip 19.2.3\n",
      "    Uninstalling pip-19.2.3:\n",
      "      Successfully uninstalled pip-19.2.3\n",
      "  Found existing installation: setuptools 41.4.0\n",
      "    Uninstalling setuptools-41.4.0:\n",
      "      Successfully uninstalled setuptools-41.4.0\n",
      "  Found existing installation: wheel 0.33.6\n",
      "    Uninstalling wheel-0.33.6:\n",
      "      Successfully uninstalled wheel-0.33.6\n",
      "Successfully installed pip-24.0 setuptools-68.0.0 wheel-0.42.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0adc8680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 11.2 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 12.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 12.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.0/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 12.3 MB/s  0:00:01\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439d380",
   "metadata": {},
   "source": [
    "<H1>Load Library</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b3ca79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07db157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0aa1e",
   "metadata": {},
   "source": [
    "<H1>Download The Models</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3dd1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shahe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740b3ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shahe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e065dfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\shahe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbf8c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahe\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#local_cache_path = \"C:/Users/shahe/.cache/huggingface/transformers/sentence-transformers/all-mpnet-base-v2\"\n",
    "#model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "#local_cache_path = \"C:/Users/shahe/.cache/huggingface/transformers/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "local_cache_path = \"C:/Users/shahe/.cache/huggingface/transformers/BAAI/bge-m3\"\n",
    "model_name = 'BAAI/bge-m3'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=local_cache_path)\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=local_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341648ee",
   "metadata": {},
   "source": [
    "<H1>Lets recognise and understand each word</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e57d5178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  add 50g of sliced onnions on Quartile 1\n",
      "PoS Tagging Result:\n",
      "add: VERB\n",
      "50: NUM\n",
      "g: NOUN\n",
      "of: ADP\n",
      "sliced: ADJ\n",
      "onnions: NOUN\n",
      "on: ADP\n",
      "Quartile: PROPN\n",
      "1: NUM\n"
     ]
    }
   ],
   "source": [
    "# this model is relatively slow but much more acurate \n",
    "\n",
    "\n",
    " \n",
    "# Load the English language model\n",
    "\n",
    "# Sample text\n",
    "text = \"add 50g of sliced onnions on Quartile 1\"\n",
    " \n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    " \n",
    "# Display the PoS tagged result\n",
    "print(\"Original Text: \", text)\n",
    "print(\"PoS Tagging Result:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0983034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{token.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc7068b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\shahe/nltk_data'\n    - 'C:\\\\Users\\\\shahe\\\\Anaconda3\\\\envs\\\\torch\\\\nltk_data'\n    - 'C:\\\\Users\\\\shahe\\\\Anaconda3\\\\envs\\\\torch\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\shahe\\\\Anaconda3\\\\envs\\\\torch\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\shahe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#this model is pretty fast but not much accurate\u001b[39;00m\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd 50g of sliced onnions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(tokens)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(pos_tags)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\shahe/nltk_data'\n    - 'C:\\\\Users\\\\shahe\\\\Anaconda3\\\\envs\\\\torch\\\\nltk_data'\n    - 'C:\\\\Users\\\\shahe\\\\Anaconda3\\\\envs\\\\torch\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\shahe\\\\Anaconda3\\\\envs\\\\torch\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\shahe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#this model is pretty fast but not much accurate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = \"add 50g of sliced onnions.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8617aa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sale', 'id', 'docid']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"Sale\", \"Id\", \"DocId\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b054978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---IGNORE THIS AND DONT RUN--\n",
    "# model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# # a) Get predictions\n",
    "# nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "\n",
    "# # b) Load model & tokenizer\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a762dcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# ---IGNORE THIS AND DONT RUN--\n",
    "\n",
    "# local_cache_path = \"C:/Users/shahe/.cache/huggingface/transformers/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad/\"\n",
    "model_name2 = \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp2 = pipeline('question-answering', model=model_name2, tokenizer=model_name2)\n",
    "\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model2 = AutoModelForQuestionAnswering.from_pretrained(model_name2)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "# , cache_dir=local_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da2e334-b1b8-4056-b046-abd63cdda44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shahe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b85e2",
   "metadata": {},
   "source": [
    "<h1>START HERE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6ac36af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ab9e1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reinforcement_Data =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "Reinforcement_Data2d= [0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "df192495",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['select * from dbo.Sales', 'select * from dbo.Customer', 'select Id from dbo.Sales','select DocNumber from dbo.Sales','select Id, DocDate from dbo.Sales','select Id, DocNumber from dbo.Sales', 'select DocDate, DocNumber from dbo.Sales','select Id, DocDate, DocNumber from dbo.Sales', 'Select * from Sales where Id = \\\\d', \"Select * from dbo.Sales where DocNumber = '\\\\d'\", \"select * from TaxSettlement /* Shows all the tax settlement */\", \"select * from TaxSettlement where DocNumber = 'TS-00000' /* Shows only the tax settlement using Document Number */\", \"select * from TaxSettlement where quarter = 0 /* Shows all the tax settlement using quarters */\", \"select TaxId, Description, Amount, Adjustment, VATAmount from TaxMaster tm join TaxSettlementItems tsi on tm.Id=tsi.TaxId join TaxSettlement Ts on Ts.Id = tsi.TaxSettlementId where Ts.DocNumber = 'TS-00000' /* Shows all the tax master or description fetching discription using document NUMBER from tax settlement*/\", \"select * from TaxSettlement where FiscalYear = '2020' /* Shows all the tax settlement for that year*/\",\"EXEC dbo.GetTaxDetailsByDocNumber 'Ts-00000' \", \"select * from TaxSettlement where TotalInputVat = 0.00 /*using Tax Settlement to get Total Input vat */\", \"select * from TaxSettlement where TotalOutputVat = 0.00 /*using Total Output vat value to get the tax settlement*/ \", \"select * from TaxSettlement where NetTotalDue = 0.00 /* using the total net vat to get the tax settlement*/\", \"select * from TaxSettlement where Status = 'saved or posted' /* using status posted or saved to show TaxSettlement */\", \"EXEC GetTaxStartingFromDate '1999-01-02' /*show Tax Settlement from date */\",\"select * from TaxSettlement where ToDate <= '1999-01-02' /*show Tax Settlement only till date or until date only */\"]\n",
    "data2d =[\"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\", \"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /*Filter TaxSettlement records with a starting date (FromDate >= some_start_date) and a specific document number (DocNumber = some_doc_number that contains TS or TS- or TS-00) [1][3]*/\", \"SELECT * FROM dbo.TaxSettlement WHERE ToDate <= 'some_end_date' AND DocNumber = 'some_doc_number' /* Filter TaxSettlement records with an end date (ToDate <= some_end_date) and a specific document number (DocNumber = some_doc_number  that contains TS or TS- or TS-00) [2][3] */ \", \"select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Query for TaxSettlement records where TotalInputVat is set to a specified value [4] and TotalOutputVat is set to another specified value [5] */\"]\n",
    "\n",
    "\n",
    "# data2d = [\n",
    "#     \"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* Retrieve TaxSettlement data filtered by a date range: starting date (FromDate) to ending date (ToDate) within a specified period [1][2] */\",\n",
    "    \n",
    "#     \"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /* Select TaxSettlement records based on a minimum start date (FromDate >= some_start_date) and a specific document identifier (DocNumber contains TS, TS-, or TS-00 patterns) [1][3] */\",\n",
    "    \n",
    "#     \"SELECT * FROM dbo.TaxSettlement WHERE ToDate <= 'some_end_date' AND DocNumber = 'some_doc_number' /* Extract TaxSettlement entries limited by an end date (ToDate) and document reference (DocNumber with TS, TS-, TS-00 pattern) for date-specific records [2][3] */\",\n",
    "    \n",
    "#     \"select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Search TaxSettlement where TotalInputVat matches 'input_value' and TotalOutputVat is 'output_value', focusing on VAT-specific filters for financial reporting [4][5] */\",\n",
    "\n",
    "#     'select * from dbo.Sales', 'select * from dbo.Customer', 'select Id from dbo.Sales','select DocNumber from dbo.Sales','select Id, DocDate from dbo.Sales','select Id, DocNumber from dbo.Sales', 'select DocDate, DocNumber from dbo.Sales','select Id, DocDate, DocNumber from dbo.Sales', 'Select * from Sales where Id = \\\\d', \"Select * from dbo.Sales where DocNumber = '\\\\d'\", \"select * from TaxSettlement /* Shows all the tax settlement */\", \"select * from TaxSettlement where DocNumber = 'TS-00000' /* Shows only the tax settlement using Document Number */\", \"select * from TaxSettlement where quarter = 0 /* Shows all the tax settlement using quarters */\", \"select TaxId, Description, Amount, Adjustment, VATAmount from TaxMaster tm join TaxSettlementItems tsi on tm.Id=tsi.TaxId join TaxSettlement Ts on Ts.Id = tsi.TaxSettlementId where Ts.DocNumber = 'TS-00000' /* Shows all the tax master or description fetching discription using document NUMBER from tax settlement*/\", \"select * from TaxSettlement where FiscalYear = '2020' /* Shows all the tax settlement for that year*/\",\"EXEC dbo.GetTaxDetailsByDocNumber 'Ts-00000' \", \"select * from TaxSettlement where TotalInputVat = 0.00 /*using Tax Settlement to get Total Input vat */\", \"select * from TaxSettlement where TotalOutputVat = 0.00 /*using Total Output vat value to get the tax settlement*/ \", \"select * from TaxSettlement where NetTotalDue = 0.00 /* using the total net vat to get the tax settlement*/\", \"select * from TaxSettlement where Status = 'saved or posted' /* using status posted or saved to show TaxSettlement */\", \"EXEC GetTaxStartingFromDate '1999-01-02' /*show Tax Settlement from date */\",\"select * from TaxSettlement where ToDate <= '1999-01-02' /*show Tax Settlement only till date or until date only */\"\n",
    "    \n",
    "# ]\n",
    "# \"EXEC dbo.GetTaxDetailsByDocNumber 'TS-00000' /*show description of the tax settlement using the document id in more detail*/\"\n",
    "\n",
    "# \"select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Query for TaxSettlement records where TotalInputVat is set to a specified value [1] and TotalOutputVat is set to another specified value [2] */\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b45c033b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5fcf5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt =\"Display all the tax Settlement\"\n",
    "\n",
    "# prompt = \"show me the tax settlement TS-00001\"\n",
    "# prompt = \"show me the tax settlement for Document TS-00001\"\n",
    "# prompt = \"get me the tax for the 3 quarter\"\n",
    "# prompt = \"how much is my tax for quarter 3\"\n",
    "# prompt = \"show me tax settlement according to the fiscal year 2024\"\n",
    "# prompt= \"show Tax Settlement for year 2020\"\n",
    "# prompt = \"show Tax Settlement for 2024\" #ERROR CAUSE THE ALGO GETS CONFUSED WHAT 2024 WE ARE TALKING ABOUT\n",
    "\n",
    "# prompt = \"show me tax settlement starting from date 2020-01-01\"\n",
    "# prompt = \"show me tax settlement from 2020-01-01\"\n",
    "\n",
    "# prompt = \"show me tax settlement till date 2024-12-12\"\n",
    "# prompt = \"show me tax settlement till 2023-12-12\"\n",
    "# prompt = \"show me tax settlement up till 2023-12-12\"\n",
    "\n",
    "\n",
    "# prompt = \"show me tax settlment for the 1 quarter\"\n",
    "# prompt = \"show me the tax settlement for quarter 1\"\n",
    "# prompt = \"display tax settlement on Quarter-1\"\n",
    "\n",
    "# prompt = \"show me tax settlement where input vat is 0.01\"\n",
    "# prompt = \"show settlement for 0.01 input vat\"\n",
    "\n",
    "# prompt = \"show me tax settlement where output vat is 0.01\"\n",
    "# prompt = \"show me tax settlement where net total is 0.01\"\n",
    "# prompt = \"show me tax settlement where total net due is 0.01\"\n",
    "\n",
    "# prompt = \"show me tax settlement where status is posted\"\n",
    "# prompt = \"show me tax settlement that has been saved\"\n",
    "\n",
    "\n",
    "# prompt = \"show me the tax settlement description for TS-00001\"\n",
    "\n",
    "# prompt= \"describe me TS-00001 in more detail\"\n",
    "\n",
    "\n",
    "\n",
    "# We can search multiple queries using the and keyword\n",
    "prompt = \"show me tax settlement from 2020-01-01 and till 2024-12-12\"\n",
    "# prompt = \"show me tax settlement from 2020-01-01 and for TS-00001\"\n",
    "\n",
    "\n",
    "# prompt = \"show me tax settlement end till 2020-01-01 and for TS-00001\"\n",
    "# prompt =  \"show me tax settlement ending till 2020-01-01 and for TS-00001\"\n",
    "\n",
    "# prompt=  \"show when input vat is 0.0 and total output vat is 2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "537c718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of 'and': 1\n"
     ]
    }
   ],
   "source": [
    "and_count = len(re.findall(r'\\band\\b', prompt))\n",
    "\n",
    "print(\"Occurrences of 'and':\", and_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6cd4dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if and_count == 0:\n",
    "    document= data\n",
    "    document.insert(0, prompt)\n",
    "else:\n",
    "    document = data2d\n",
    "    # document.append(data)\n",
    "    document.insert(0, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9ebc0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show me tax settlement from 2020-01-01 and till 2024-12-12',\n",
       " \"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\",\n",
       " \"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /*Filter TaxSettlement records with a starting date (FromDate >= some_start_date) and a specific document number (DocNumber = some_doc_number that contains TS or TS- or TS-00) [1][3]*/\",\n",
       " \"SELECT * FROM dbo.TaxSettlement WHERE ToDate <= 'some_end_date' AND DocNumber = 'some_doc_number' /* Filter TaxSettlement records with an end date (ToDate <= some_end_date) and a specific document number (DocNumber = some_doc_number  that contains TS or TS- or TS-00) [2][3] */ \",\n",
       " \"select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Query for TaxSettlement records where TotalInputVat is set to a specified value [4] and TotalOutputVat is set to another specified value [5] */\"]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "605968f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['show tax settlement 2020-01-01 till 2024-12-12',\n",
       " \"select * dbo.taxsettlement fromdate >= 'some_start_date' todate <= 'some_end_date' /* query retrieves data starting date (fromdate) ending date (todate), covering range start end date [1][2] */\",\n",
       " \"select * dbo.taxsettlement fromdate >= 'some_start_date' docnumber= 'some_doc_number'; /*filter taxsettlement records starting date (fromdate >= some_start_date) specific document number (docnumber = some_doc_number contains ts ts- ts-00) [1][3]*/\",\n",
       " \"select * dbo.taxsettlement todate <= 'some_end_date' docnumber = 'some_doc_number' /* filter taxsettlement records end date (todate <= some_end_date) specific document number (docnumber = some_doc_number contains ts ts- ts-00) [2][3] */\",\n",
       " \"select * dbo.taxsettlement totalinputvat = 'input_value' totaloutputvat = 'output_value' /* query taxsettlement records totalinputvat set specified value [4] totaloutputvat set another specified value [5] */\"]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))\n",
    "# processed_docs2 = [doc.lower().replace(stopwords.words(\"english\"),\"\") for doc in document]\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "processed_docs2 = [' '.join([word for word in doc.lower().split() if word not in stop_words]) for doc in document]\n",
    "# processed_docs2 = [doc.lower().replace(stopwords.words(\"english\"), '') for doc in document]\n",
    "processed_docs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a6d807f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "vectorized = tfidf.fit_transform(processed_docs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd9072b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.69314718 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 1.69314718 2.09861229 2.09861229 1.40546511 1.18232156 1.69314718\n",
      " 1.69314718 1.69314718 2.09861229 1.69314718 1.69314718 2.09861229\n",
      " 1.69314718 2.09861229 1.69314718 2.09861229 1.40546511 2.09861229\n",
      " 1.18232156 2.09861229 2.09861229 2.09861229 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 2.09861229 2.09861229 1.69314718 2.09861229\n",
      " 1.18232156 2.09861229 1.69314718 2.09861229 2.09861229 1.69314718\n",
      " 2.09861229]\n",
      "----------\n",
      "All words in the vocabulary ['00' '01' '12' '2020' '2024' 'another' 'contains' 'covering' 'data'\n",
      " 'date' 'dbo' 'docnumber' 'document' 'end' 'ending' 'filter' 'fromdate'\n",
      " 'input_value' 'number' 'output_value' 'query' 'range' 'records'\n",
      " 'retrieves' 'select' 'set' 'settlement' 'show' 'some_doc_number'\n",
      " 'some_end_date' 'some_start_date' 'specific' 'specified' 'start'\n",
      " 'starting' 'tax' 'taxsettlement' 'till' 'todate' 'totalinputvat'\n",
      " 'totaloutputvat' 'ts' 'value']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names_out())\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d14baaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF representation for all documents in our corpus\n",
      " [[0.         0.53452248 0.53452248 0.26726124 0.26726124 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26726124 0.26726124 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26726124\n",
      "  0.         0.26726124 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.22674203 0.22674203 0.45555533 0.1277425  0.\n",
      "  0.         0.18293404 0.22674203 0.         0.36586808 0.\n",
      "  0.         0.         0.18293404 0.22674203 0.         0.22674203\n",
      "  0.1277425  0.         0.         0.         0.         0.18293404\n",
      "  0.18293404 0.         0.         0.22674203 0.18293404 0.\n",
      "  0.1277425  0.         0.36586808 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.1659678  0.         0.         0.         0.         0.\n",
      "  0.1659678  0.         0.         0.13776826 0.11589501 0.33193559\n",
      "  0.1659678  0.         0.         0.1659678  0.33193559 0.\n",
      "  0.1659678  0.         0.         0.         0.13776826 0.\n",
      "  0.11589501 0.         0.         0.         0.33193559 0.\n",
      "  0.33193559 0.1659678  0.         0.         0.1659678  0.\n",
      "  0.23179001 0.         0.         0.         0.         0.49790339\n",
      "  0.        ]\n",
      " [0.1659678  0.         0.         0.         0.         0.\n",
      "  0.1659678  0.         0.         0.13776826 0.11589501 0.33193559\n",
      "  0.1659678  0.1659678  0.         0.1659678  0.         0.\n",
      "  0.1659678  0.         0.         0.         0.13776826 0.\n",
      "  0.11589501 0.         0.         0.         0.33193559 0.33193559\n",
      "  0.         0.1659678  0.         0.         0.         0.\n",
      "  0.23179001 0.         0.33193559 0.         0.         0.49790339\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.19610169\n",
      "  0.         0.         0.         0.         0.11048027 0.\n",
      "  0.         0.         0.         0.         0.         0.19610169\n",
      "  0.         0.19610169 0.15821361 0.         0.13133159 0.\n",
      "  0.11048027 0.39220339 0.         0.         0.         0.\n",
      "  0.         0.         0.39220339 0.         0.         0.\n",
      "  0.22096055 0.         0.         0.39220339 0.39220339 0.\n",
      "  0.39220339]]\n",
      "----------\n",
      "Tfidf representation for prompt:\n",
      " [[0.         0.53452248 0.53452248 0.26726124 0.26726124 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26726124 0.26726124 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26726124\n",
      "  0.         0.26726124 0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TFIDF representation for all documents in our corpus\\n\",vectorized.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([prompt])\n",
    "print(\"Tfidf representation for prompt:\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "554f74f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :\n",
      "Euclidean Distance: (1, 1.4142135623730951)\n",
      "2 :\n",
      "Euclidean Distance: (2, 1.4142135623730951)\n",
      "3 :\n",
      "Euclidean Distance: (3, 1.4142135623730951)\n",
      "4 :\n",
      "Euclidean Distance: (4, 1.4142135623730951)\n",
      "[(1, 1.4142135623730951), (2, 1.4142135623730951), (3, 1.4142135623730951), (4, 1.4142135623730951)]\n",
      "check\n",
      "       Distance\n",
      "Index          \n",
      "1      1.414214\n",
      "2      1.414214\n",
      "3      1.414214\n",
      "4      1.414214\n",
      "After update\n",
      "[1, 2, 3, 4]\n",
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\n",
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /*Filter TaxSettlement records with a starting date (FromDate >= some_start_date) and a specific document number (DocNumber = some_doc_number that contains TS or TS- or TS-00) [1][3]*/\n",
      "SELECT * FROM dbo.TaxSettlement WHERE ToDate <= 'some_end_date' AND DocNumber = 'some_doc_number' /* Filter TaxSettlement records with an end date (ToDate <= some_end_date) and a specific document number (DocNumber = some_doc_number  that contains TS or TS- or TS-00) [2][3] */ \n"
     ]
    }
   ],
   "source": [
    "mat = vectorized.toarray()\n",
    "count =1\n",
    "distance= []\n",
    "\n",
    "while len(processed_docs2) > count:\n",
    "    print(count, \":\")\n",
    "    distance.append((count ,euclidean(mat[0], mat[count])))\n",
    "    print(\"Euclidean Distance:\", distance[count-1])\n",
    "    count+=1;\n",
    "\n",
    "\n",
    "\n",
    "distance.sort(key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# Get the top 3 highest similarity scores\n",
    "\n",
    "top_3_index_tf = distance[:3]\n",
    "print(distance)\n",
    "count=3\n",
    "while( count<len(distance) and top_3_index_tf[2][1] == distance[count][1]):\n",
    "    top_3_index_tf.append(distance[count])\n",
    "    count+=1\n",
    "      \n",
    "index_tfidf=[]\n",
    "print(\"check\")\n",
    "df_tf = pd.DataFrame(top_3_index_tf, columns=['Index', 'Distance'])\n",
    "\n",
    "# Set the 'Index' column as the DataFrame's index\n",
    "df_tf.set_index('Index', inplace=True)\n",
    "print(df_tf)\n",
    "for item in top_3_index_tf:\n",
    "    index_tfidf.append(item[0])  # item[0] is the index from the tuple\n",
    "\n",
    "# Display the updated index (which now contains only the index values)\n",
    "print(\"After update\")\n",
    "print(index_tfidf)\n",
    "\n",
    "\n",
    "sentences_tfidf = []\n",
    "count=0;\n",
    "while count<3:\n",
    "    sentences_tfidf.append(document[index_tfidf[count]])\n",
    "    print(document[index_tfidf[count]])\n",
    "    count+=1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a78be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "428363df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Distance\n",
      "Index          \n",
      "1      1.414214\n",
      "2      1.414214\n",
      "3      1.414214\n",
      "4      1.414214\n"
     ]
    }
   ],
   "source": [
    "print(df_tf)\n",
    "# distance_at_index = df_tf.loc[17, 'Distance']\n",
    "# print(distance_at_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4d207078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show me tax settlement from 2020-01-01 and till 2024-12-12',\n",
       " \"select * from dbo.taxsettlement where fromdate >= 'some_start_date' and todate <= 'some_end_date' /* this query retrieves data with both a starting date (fromdate) and an ending date (todate), covering a range from the start to end date [1][2] */\",\n",
       " \"select * from dbo.taxsettlement where fromdate >= 'some_start_date' and docnumber= 'some_doc_number'; /*filter taxsettlement records with a starting date (fromdate >= some_start_date) and a specific document number (docnumber = some_doc_number that contains ts or ts- or ts-00) [1][3]*/\",\n",
       " \"select * from dbo.taxsettlement where todate <= 'some_end_date' and docnumber = 'some_doc_number' /* filter taxsettlement records with an end date (todate <= some_end_date) and a specific document number (docnumber = some_doc_number  that contains ts or ts- or ts-00) [2][3] */ \",\n",
       " \"select * from dbo.taxsettlement where totalinputvat = 'input_value' and totaloutputvat = 'output_value' /* query for taxsettlement records where totalinputvat is set to a specified value [4] and totaloutputvat is set to another specified value [5] */\"]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = [doc.lower()for doc in document]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc076824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>12</th>\n",
       "      <th>2020</th>\n",
       "      <th>2024</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>another</th>\n",
       "      <th>both</th>\n",
       "      <th>contains</th>\n",
       "      <th>...</th>\n",
       "      <th>this</th>\n",
       "      <th>till</th>\n",
       "      <th>to</th>\n",
       "      <th>todate</th>\n",
       "      <th>totalinputvat</th>\n",
       "      <th>totaloutputvat</th>\n",
       "      <th>ts</th>\n",
       "      <th>value</th>\n",
       "      <th>where</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>show me tax settlement from 2020-01-01 and till 2024-12-12</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select * from dbo.taxsettlement where fromdate &gt;= 'some_start_date' and todate &lt;= 'some_end_date' /* this query retrieves data with both a starting date (fromdate) and an ending date (todate), covering a range from the start to end date [1][2] */</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select * from dbo.taxsettlement where fromdate &gt;= 'some_start_date' and docnumber= 'some_doc_number'; /*filter taxsettlement records with a starting date (fromdate &gt;= some_start_date) and a specific document number (docnumber = some_doc_number that contains ts or ts- or ts-00) [1][3]*/</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select * from dbo.taxsettlement where todate &lt;= 'some_end_date' and docnumber = 'some_doc_number' /* filter taxsettlement records with an end date (todate &lt;= some_end_date) and a specific document number (docnumber = some_doc_number  that contains ts or ts- or ts-00) [2][3] */</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select * from dbo.taxsettlement where totalinputvat = 'input_value' and totaloutputvat = 'output_value' /* query for taxsettlement records where totalinputvat is set to a specified value [4] and totaloutputvat is set to another specified value [5] */</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    00  01  12  2020  2024  \\\n",
       "show me tax settlement from 2020-01-01 and till...   0   2   2     1     1   \n",
       "select * from dbo.taxsettlement where fromdate ...   0   0   0     0     0   \n",
       "select * from dbo.taxsettlement where fromdate ...   1   0   0     0     0   \n",
       "select * from dbo.taxsettlement where todate <=...   1   0   0     0     0   \n",
       "select * from dbo.taxsettlement where totalinpu...   0   0   0     0     0   \n",
       "\n",
       "                                                    an  and  another  both  \\\n",
       "show me tax settlement from 2020-01-01 and till...   0    1        0     0   \n",
       "select * from dbo.taxsettlement where fromdate ...   1    2        0     1   \n",
       "select * from dbo.taxsettlement where fromdate ...   0    2        0     0   \n",
       "select * from dbo.taxsettlement where todate <=...   1    2        0     0   \n",
       "select * from dbo.taxsettlement where totalinpu...   0    2        1     0   \n",
       "\n",
       "                                                    contains  ...  this  till  \\\n",
       "show me tax settlement from 2020-01-01 and till...         0  ...     0     1   \n",
       "select * from dbo.taxsettlement where fromdate ...         0  ...     1     0   \n",
       "select * from dbo.taxsettlement where fromdate ...         1  ...     0     0   \n",
       "select * from dbo.taxsettlement where todate <=...         1  ...     0     0   \n",
       "select * from dbo.taxsettlement where totalinpu...         0  ...     0     0   \n",
       "\n",
       "                                                    to  todate  totalinputvat  \\\n",
       "show me tax settlement from 2020-01-01 and till...   0       0              0   \n",
       "select * from dbo.taxsettlement where fromdate ...   1       2              0   \n",
       "select * from dbo.taxsettlement where fromdate ...   0       0              0   \n",
       "select * from dbo.taxsettlement where todate <=...   0       2              0   \n",
       "select * from dbo.taxsettlement where totalinpu...   2       0              2   \n",
       "\n",
       "                                                    totaloutputvat  ts  value  \\\n",
       "show me tax settlement from 2020-01-01 and till...               0   0      0   \n",
       "select * from dbo.taxsettlement where fromdate ...               0   0      0   \n",
       "select * from dbo.taxsettlement where fromdate ...               0   3      0   \n",
       "select * from dbo.taxsettlement where todate <=...               0   3      0   \n",
       "select * from dbo.taxsettlement where totalinpu...               2   0      2   \n",
       "\n",
       "                                                    where  with  \n",
       "show me tax settlement from 2020-01-01 and till...      0     0  \n",
       "select * from dbo.taxsettlement where fromdate ...      1     1  \n",
       "select * from dbo.taxsettlement where fromdate ...      1     1  \n",
       "select * from dbo.taxsettlement where todate <=...      1     1  \n",
       "select * from dbo.taxsettlement where totalinpu...      2     0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names_out(), \n",
    "                  index=processed_docs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e47807ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.14303971 0.10090092 0.09994449 0.10394384]\n",
      " [0.14303971 1.         0.44982345 0.46581234 0.31594705]\n",
      " [0.10090092 0.44982345 1.         0.81908477 0.2773501 ]\n",
      " [0.09994449 0.46581234 0.81908477 1.         0.27472113]\n",
      " [0.10394384 0.31594705 0.2773501  0.27472113 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity=cosine_similarity(df, df)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "edadeff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second maximum similarity: 0.1430397079704303\n",
      "Second maximum similarity: 0.1430397079704303\n",
      "0.07151985398521515\n",
      "show me tax settlement from 2020-01-01 and till 2024-12-12\n",
      "1.0\n",
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\n",
      "0.1430397079704303\n",
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /*Filter TaxSettlement records with a starting date (FromDate >= some_start_date) and a specific document number (DocNumber = some_doc_number that contains TS or TS- or TS-00) [1][3]*/\n",
      "0.10090091909944687\n",
      "SELECT * FROM dbo.TaxSettlement WHERE ToDate <= 'some_end_date' AND DocNumber = 'some_doc_number' /* Filter TaxSettlement records with an end date (ToDate <= some_end_date) and a specific document number (DocNumber = some_doc_number  that contains TS or TS- or TS-00) [2][3] */ \n",
      "0.09994449069791544\n",
      "select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Query for TaxSettlement records where TotalInputVat is set to a specified value [4] and TotalOutputVat is set to another specified value [5] */\n",
      "0.10394383930128556\n",
      "5\n",
      "[(0, 1.0), (1, 0.1430397079704303), (2, 0.10090091909944687), (3, 0.09994449069791544), (4, 0.10394383930128556)]\n",
      "0\n",
      "check\n",
      "0.1430397079704303\n",
      "After update\n",
      "[0, 1, 4, 2]\n",
      "show me tax settlement from 2020-01-01 and till 2024-12-12\n",
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\n",
      "select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Query for TaxSettlement records where TotalInputVat is set to a specified value [4] and TotalOutputVat is set to another specified value [5] */\n",
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /*Filter TaxSettlement records with a starting date (FromDate >= some_start_date) and a specific document number (DocNumber = some_doc_number that contains TS or TS- or TS-00) [1][3]*/\n"
     ]
    }
   ],
   "source": [
    "index= []\n",
    "count = 0\n",
    "sentences=[]\n",
    "first_row = similarity[0]\n",
    "\n",
    "# Convert it to a list and remove the maximum value (which is 1.0, the self-similarity)\n",
    "sorted_row = sorted(first_row, reverse=True)\n",
    "\n",
    "# The second value in the sorted list is the second maximum\n",
    "second_max = sorted_row[1]\n",
    "\n",
    "print(\"Second maximum similarity:\", second_max)\n",
    "\n",
    "\n",
    "# maxim= second_max - (second_max/10)\n",
    "# print(maxim)\n",
    "# while(count < len(processed_docs)):\n",
    "#     if(similarity[0][count] > maxim):\n",
    "#         index.append(count)\n",
    "#     count = count+1\n",
    "\n",
    "# print(len(index))\n",
    "# print(index[1])\n",
    "\n",
    "print(\"Second maximum similarity:\", second_max)\n",
    "\n",
    "\n",
    "maxim= second_max - (second_max/2)\n",
    "print(maxim)\n",
    "while(count < len(processed_docs)):\n",
    "    if(similarity[0][count] > maxim):\n",
    "        # sentences.append(document[count])\n",
    "        print(document[count])\n",
    "        print(similarity[0][count])\n",
    "        index.append((count, similarity[0][count]))\n",
    "    count = count+1\n",
    "\n",
    "print(len(index))\n",
    "print(index)\n",
    "\n",
    "print(index[0][0])\n",
    "\n",
    "index.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 3 highest similarity scores\n",
    "top_3_index = index[:4]\n",
    "\n",
    "new_data_df = pd.DataFrame(top_3_index, columns=['Index', 'Cosine'])\n",
    "new_data_df.set_index('Index', inplace=True)\n",
    "\n",
    "\n",
    "index=[]\n",
    "print(\"check\")\n",
    "print(top_3_index[1][1])\n",
    "\n",
    "for item in top_3_index:\n",
    "    index.append(item[0])  # item[0] is the index from the tuple\n",
    "\n",
    "# Display the updated index (which now contains only the index values)\n",
    "print(\"After update\")\n",
    "print(index)\n",
    "\n",
    "count=0;\n",
    "while count<4 and count<len(index):\n",
    "    if count ==0 or index[count] in index_tfidf:\n",
    "        sentences.append(document[index[count]])\n",
    "        print(document[index[count]])\n",
    "    count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "30835663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cosine</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.143040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.103944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Cosine\n",
       "Index          \n",
       "0      1.000000\n",
       "1      0.143040\n",
       "4      0.103944\n",
       "2      0.100901"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb128dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf['Cosine'] = df_tf.index.map(new_data_df['Cosine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "37322c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Cosine</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.143040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.100901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.103944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Distance    Cosine\n",
       "Index                    \n",
       "1      1.414214  0.143040\n",
       "2      1.414214  0.100901\n",
       "3      1.414214       NaN\n",
       "4      1.414214  0.103944"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(df_tf.loc[index[3],'Distance'])\n",
    "# print(df_tf.loc[index[3],'Cosine'])\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9791dd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Cosine</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.143040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.100901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.103944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Distance    Cosine\n",
       "Index                    \n",
       "1      1.414214  0.143040\n",
       "2      1.414214  0.100901\n",
       "3      1.414214       NaN\n",
       "4      1.414214  0.103944"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "36faec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_cosine = df_tf['Cosine'].mean()\n",
    "df_tf['Cosine'].fillna(0, inplace=True)\n",
    "# Replace NaN values in the 'Cosine' column with the calculated average\n",
    "df_tf['Cosine'].fillna(average_cosine, inplace=True)\n",
    "df_tf['Cosine'] = df_tf['Cosine'].replace(0, average_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "079b5422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Cosine</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.143040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.100901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.115961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.103944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Distance    Cosine\n",
       "Index                    \n",
       "1      1.414214  0.143040\n",
       "2      1.414214  0.100901\n",
       "3      1.414214  0.115961\n",
       "4      1.414214  0.103944"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "04116227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 2]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "26e57a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2= []\n",
    "for ind in index:\n",
    "    if ind in df_tf.index:\n",
    "        index2.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9bd1ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index2\n",
    "index.insert(0,0)\n",
    "nan_indices = df_tf[df_tf['Cosine'] == average_cosine].index.tolist()\n",
    "index.extend(nan_indices)\n",
    "\n",
    "count =0;\n",
    "while count < len(nan_indices):\n",
    "    sentences.append(document[nan_indices[count]])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1ae7f10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 2, 3]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "64a36731",
   "metadata": {},
   "outputs": [],
   "source": [
    "Return = document[index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "131a94e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show me tax settlement from 2020-01-01 and till 2024-12-12', \"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\", \"select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Query for TaxSettlement records where TotalInputVat is set to a specified value [4] and TotalOutputVat is set to another specified value [5] */\", \"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /*Filter TaxSettlement records with a starting date (FromDate >= some_start_date) and a specific document number (DocNumber = some_doc_number that contains TS or TS- or TS-00) [1][3]*/\", \"SELECT * FROM dbo.TaxSettlement WHERE ToDate <= 'some_end_date' AND DocNumber = 'some_doc_number' /* Filter TaxSettlement records with an end date (ToDate <= some_end_date) and a specific document number (DocNumber = some_doc_number  that contains TS or TS- or TS-00) [2][3] */ \"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8b5f2337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\n"
     ]
    }
   ],
   "source": [
    "print(Return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a60f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2801c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR COSINE\n",
    "# #Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# # Sentences we want sentence embeddings for\n",
    "# # Load model from HuggingFace Hub\n",
    "# # tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "# # model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# # Tokenize sentences\n",
    "# encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "# print(sentence_embeddings)\n",
    "# # Normalize embeddings\n",
    "# sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "# max = 0;\n",
    "# counter=1;\n",
    "# value=0;\n",
    "# Possible_Sentence=\"check\"\n",
    "# while(len(sentences) > counter):\n",
    "#     print(\"Sentence embedding:\")\n",
    "#     print( (sentence_embeddings[0] @ sentence_embeddings[counter]).item() )\n",
    "#     print(\"Sentence Embeding after adding Cosine weight\")\n",
    "#     val = (sentence_embeddings[0] @ sentence_embeddings[counter]).item() * (top_3_index[counter][1])\n",
    "#     print( val )\n",
    "#     if(val >= max):\n",
    "#         max=val\n",
    "#         value=counter;\n",
    "#         print(\"Max\")\n",
    "#         print(max)\n",
    "#         Possible_Sentence = sentences[counter];\n",
    "        \n",
    "#     counter = counter +1\n",
    "\n",
    "# print(Possible_Sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "45489713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.1148e-01, -2.7302e-01, -1.1989e+00,  ..., -3.7442e-01,\n",
      "         -2.2645e+00, -1.5940e+00],\n",
      "        [-1.7505e-03, -3.2714e-01, -6.2417e-01,  ..., -9.7613e-02,\n",
      "         -1.4323e+00, -1.4271e+00],\n",
      "        [-3.3032e-01, -1.1617e-01, -1.1083e+00,  ..., -2.6961e-01,\n",
      "         -8.3562e-01, -6.3514e-01],\n",
      "        [ 1.5729e-02, -9.8379e-01, -1.1954e+00,  ..., -1.1812e-01,\n",
      "         -1.1097e+00, -1.1041e+00],\n",
      "        [-2.4819e-01, -9.0160e-01, -1.0094e+00,  ..., -3.1533e-02,\n",
      "         -1.2010e+00, -1.0209e+00]])\n",
      "Sentence embedding:\n",
      "0.6927957534790039\n",
      "Sentence Embeding after adding Cosine weight and tfidf\n",
      "0.04954865113039546\n",
      "Max\n",
      "0.04954865113039546\n",
      "Sentence embedding:\n",
      "0.6529651284217834\n",
      "Sentence Embeding after adding Cosine weight and tfidf\n",
      "0.03393585118900857\n",
      "Sentence embedding:\n",
      "0.6774196624755859\n",
      "Sentence Embeding after adding Cosine weight and tfidf\n",
      "0.03417613327991185\n",
      "Sentence embedding:\n",
      "0.6682919263839722\n",
      "Sentence Embeding after adding Cosine weight and tfidf\n",
      "0.038748063365040754\n",
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\n"
     ]
    }
   ],
   "source": [
    "#using tfidf vectors also now\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "NotSatisfiedList = []\n",
    "# Sentences we want sentence embeddings for\n",
    "# Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings)\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "max = 0;\n",
    "counter=1;\n",
    "value=0;\n",
    "Possible_Sentence=\"check\"\n",
    "while(len(sentences) > counter):\n",
    "    print(\"Sentence embedding:\")\n",
    "    print( (sentence_embeddings[0] @ sentence_embeddings[counter]).item() )\n",
    "    print(\"Sentence Embeding after adding Cosine weight and tfidf\")\n",
    "    val = ((sentence_embeddings[0] @ sentence_embeddings[counter]).item() * df_tf.loc[index[counter],'Cosine'])/ (df_tf.loc[index[counter],'Distance'] ** 2)\n",
    "    print( val )\n",
    "    if(val >= max):\n",
    "        max=val\n",
    "        value=counter;\n",
    "        print(\"Max\")\n",
    "        print(max)\n",
    "        Possible_Sentence = sentences[counter];\n",
    "\n",
    "    NotSatisfiedList.append((sentences[counter], val)) \n",
    "    counter = counter +1\n",
    "\n",
    "print(Possible_Sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0be8ec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\n"
     ]
    }
   ],
   "source": [
    "print(Possible_Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d148717",
   "metadata": {},
   "outputs": [],
   "source": [
    "NotSatisfiedList\n",
    "NotSatisfiedList.sort(key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1c44af7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\",\n",
       "  0.04954865113039546),\n",
       " (\"SELECT * FROM dbo.TaxSettlement WHERE ToDate <= 'some_end_date' AND DocNumber = 'some_doc_number' /* Filter TaxSettlement records with an end date (ToDate <= some_end_date) and a specific document number (DocNumber = some_doc_number  that contains TS or TS- or TS-00) [2][3] */ \",\n",
       "  0.038748063365040754),\n",
       " (\"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /*Filter TaxSettlement records with a starting date (FromDate >= some_start_date) and a specific document number (DocNumber = some_doc_number that contains TS or TS- or TS-00) [1][3]*/\",\n",
       "  0.03417613327991185),\n",
       " (\"select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Query for TaxSettlement records where TotalInputVat is set to a specified value [4] and TotalOutputVat is set to another specified value [5] */\",\n",
       "  0.03393585118900857)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NotSatisfiedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5d7e0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = \"no\"\n",
    "# count =1\n",
    "\n",
    "# while(ans == 'no' and count <= 3):\n",
    "#     match = re.search(r'/\\*(.*?)\\*/', Possible_Sentence)\n",
    "    \n",
    "#     if match:\n",
    "#         print(match.group(1))  # This will print the text between /* and */\n",
    "#         while True:\n",
    "#             ans = input(\"Please type 'yes' or 'no': \").lower()\n",
    "#             if ans in ['yes', 'no']:\n",
    "#                 break  # Exit the loop if valid input is entered\n",
    "#             else:\n",
    "#                 print(\"Invalid input. Please type 'yes' or 'no'.\")\n",
    "#         if(ans == 'no' and count < 3):\n",
    "#             Possible_Sentence = str(NotSatisfiedList[count])\n",
    "#         count +=1\n",
    "#     else:\n",
    "#         print(\"No match found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "31e45cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchDataIntoSql(Possible_Sentence):\n",
    "    \n",
    "    \n",
    "    if(and_count ==0):\n",
    "        # Example query\n",
    "        query = Possible_Sentence\n",
    "        \n",
    "        # Check if the query contains the literal '\\d'\n",
    "        pattern = r\"\\d|'TS-\\d+'|'posted'|'saved or posted'\"  # Use double backslashes to escape the backslash\n",
    "        \n",
    "        if re.search(pattern, query):\n",
    "            print(\"The query contains the literal '\\\\d'.\")\n",
    "                \n",
    "            if re.search(r\"TS-\\d\",query):\n",
    "                    print(\"DocNumber\")\n",
    "                    replacement_invoice= re.search(r\"TS-\\d+\",prompt, re.IGNORECASE)\n",
    "                    if replacement_invoice:\n",
    "                        print(\"DocNumber:\",replacement_invoice.group() )  # This will print \"TS-12345\"\n",
    "                        replacement_invoice = replacement_invoice.group()\n",
    "                        updated_query = re.sub(r\"TS-\\d+\", replacement_invoice, query)\n",
    "                        print(\"Updated query:\", updated_query)\n",
    "                \n",
    "            elif re.search(r\"saved|posted\", query):\n",
    "                replacement_number = re.search(r\"saved|posted\", prompt)\n",
    "                if replacement_number:\n",
    "                        replacement_number_text = replacement_number.group()  # Extract the matched text as a string\n",
    "                        print(\"Value:\", replacement_number_text)\n",
    "                        updated_query = re.sub(r\"saved or posted\", replacement_number_text, query)\n",
    "                        print(\"Updated query:\", updated_query)\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                replacement_number = re.search(r\"\\d{4}-\\d{2}-\\d{2}|\\d+.\\d+|\\d+\", prompt)\n",
    "                if replacement_number:\n",
    "                        replacement_number_text = replacement_number.group()  # Extract the matched text as a string\n",
    "                        print(\"Value:\", replacement_number_text)\n",
    "                        updated_query = re.sub(r\"\\d{4}-\\d{2}-\\d{2}|\\d+.\\d+|\\d+\", replacement_number_text, query)\n",
    "                        print(\"Updated query:\", updated_query)\n",
    "        else:\n",
    "            print(\"The query does not contain the literal '\\\\d'.\")\n",
    "            updated_query = query\n",
    "            return updated_query\n",
    "    \n",
    "    else:\n",
    "        values = re.findall(r'\\[(\\d+)\\]', Possible_Sentence)\n",
    "    \n",
    "        # Convert each matched value to an integer\n",
    "        values = [int(value) for value in values]\n",
    "        print(values)\n",
    "        \n",
    "        questions = ['what is the year', ['what is the starting date','some_start_date'], ['what is the ending date','some_end_date'],['what is the document number','some_doc_number'] , ['what is the total input vat','input_value'], ['what is the total output vat', 'output_value'],'what is the total net vat']\n",
    "    \n",
    "        print(\"multiple value\")\n",
    "        for val in values:\n",
    "            print(val)\n",
    "            QA_input = {\n",
    "                'question': questions[val][0],\n",
    "                'context': prompt\n",
    "            }\n",
    "            res = nlp2(QA_input)\n",
    "            print(res)\n",
    "            print(res['answer'])\n",
    "            answer = res['answer']\n",
    "            \n",
    "            print(questions[val][1])\n",
    "            updated_query = Possible_Sentence.replace(questions[val][1], answer)\n",
    "            Possible_Sentence= updated_query\n",
    "        print(updated_query)\n",
    "        return updated_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d073c959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "multiple value\n",
      "1\n",
      "{'score': 0.931424081325531, 'start': 28, 'end': 38, 'answer': '2020-01-01'}\n",
      "2020-01-01\n",
      "some_start_date\n",
      "2\n",
      "{'score': 0.993730366230011, 'start': 48, 'end': 58, 'answer': '2024-12-12'}\n",
      "2024-12-12\n",
      "some_end_date\n",
      "select * from dbo.TaxSettlement where FromDate >= '2020-01-01' and ToDate <= '2024-12-12' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"select * from dbo.TaxSettlement where FromDate >= '2020-01-01' and ToDate <= '2024-12-12' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# if(and_count ==0):\n",
    "#     # Example query\n",
    "#     query = Possible_Sentence\n",
    "    \n",
    "#     # Check if the query contains the literal '\\d'\n",
    "#     pattern = r\"\\d|'TS-\\d+'|'posted'|'saved or posted'\"  # Use double backslashes to escape the backslash\n",
    "    \n",
    "#     if re.search(pattern, query):\n",
    "#         print(\"The query contains the literal '\\\\d'.\")\n",
    "            \n",
    "#         if re.search(r\"TS-\\d\",query):\n",
    "#                 print(\"DocNumber\")\n",
    "#                 replacement_invoice= re.search(r\"TS-\\d+\",prompt, re.IGNORECASE)\n",
    "#                 if replacement_invoice:\n",
    "#                     print(\"DocNumber:\",replacement_invoice.group() )  # This will print \"TS-12345\"\n",
    "#                     replacement_invoice = replacement_invoice.group()\n",
    "#                     updated_query = re.sub(r\"TS-\\d+\", replacement_invoice, query)\n",
    "#                     print(\"Updated query:\", updated_query)\n",
    "            \n",
    "#         elif re.search(r\"saved|posted\", query):\n",
    "#             replacement_number = re.search(r\"saved|posted\", prompt)\n",
    "#             if replacement_number:\n",
    "#                     replacement_number_text = replacement_number.group()  # Extract the matched text as a string\n",
    "#                     print(\"Value:\", replacement_number_text)\n",
    "#                     updated_query = re.sub(r\"saved or posted\", replacement_number_text, query)\n",
    "#                     print(\"Updated query:\", updated_query)\n",
    "                \n",
    "#         else:\n",
    "            \n",
    "#             replacement_number = re.search(r\"\\d{4}-\\d{2}-\\d{2}|\\d+.\\d+|\\d+\", prompt)\n",
    "#             if replacement_number:\n",
    "#                     replacement_number_text = replacement_number.group()  # Extract the matched text as a string\n",
    "#                     print(\"Value:\", replacement_number_text)\n",
    "#                     updated_query = re.sub(r\"\\d{4}-\\d{2}-\\d{2}|\\d+.\\d+|\\d+\", replacement_number_text, query)\n",
    "#                     print(\"Updated query:\", updated_query)\n",
    "#     else:\n",
    "#         print(\"The query does not contain the literal '\\\\d'.\")\n",
    "#         updated_query = query\n",
    "\n",
    "# else:\n",
    "#     values = re.findall(r'\\[(\\d+)\\]', Possible_Sentence)\n",
    "\n",
    "#     # Convert each matched value to an integer\n",
    "#     values = [int(value) for value in values]\n",
    "#     print(values)\n",
    "    \n",
    "#     questions = ['what is the year', ['what is the starting date','some_start_date'], ['what is the ending date','some_end_date'],['what is the document number','some_doc_number'] ,'what is the total net vat', 'what is the total output vat', 'what is the total input vat']\n",
    "\n",
    "#     print(\"multiple value\")\n",
    "#     for val in values:\n",
    "#         print(val)\n",
    "#         QA_input = {\n",
    "#             'question': questions[val][0],\n",
    "#             'context': prompt\n",
    "#         }\n",
    "#         res = nlp2(QA_input)\n",
    "#         print(res)\n",
    "#         print(res['answer'])\n",
    "#         answer = res['answer']\n",
    "        \n",
    "#         print(questions[val][1])\n",
    "#         updated_query = Possible_Sentence.replace(questions[val][1], answer)\n",
    "#         Possible_Sentence= updated_query\n",
    "#     print(updated_query)\n",
    "\n",
    "FetchDataIntoSql(Possible_Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c75f4270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you satisfied\n",
      " This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type 'yes' or 'no':  yes\n"
     ]
    }
   ],
   "source": [
    "ans = \"no\"\n",
    "count =1\n",
    "print(\"Are you satisfied\")\n",
    "while(ans == 'no' and count <= 3):\n",
    "    match = re.search(r'/\\*(.*?)\\*/', Possible_Sentence)\n",
    "    \n",
    "    if match:\n",
    "        print(match.group(1))  # This will print the text between /* and */\n",
    "        while True:\n",
    "            ans = input(\"Please type 'yes' or 'no': \").lower()\n",
    "            if ans in ['yes', 'no']:\n",
    "                break  # Exit the loop if valid input is entered\n",
    "            else:\n",
    "                print(\"Invalid input. Please type 'yes' or 'no'.\")\n",
    "        if(ans == 'no' and count < 3):\n",
    "            Possible_Sentence = str(NotSatisfiedList[count][0])\n",
    "            count +=1\n",
    "\n",
    "    else:\n",
    "        print(\"No match found\")\n",
    "        break;\n",
    "\n",
    "\n",
    "if ans == 'yes' and count > 1:\n",
    "    FetchDataIntoSql(Possible_Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cdb347e7-1b95-4341-a804-08a859dbca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Query Show me all tax settlements from Quarter 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "IDF for all words in the vocabulary [2.79175947 2.79175947 3.07944154 3.07944154 3.07944154 3.48490665\n",
      " 3.48490665 3.48490665 3.48490665 3.07944154 1.78015856 3.48490665\n",
      " 3.48490665 2.79175947 2.09861229 3.07944154 3.07944154 3.48490665\n",
      " 3.48490665 2.79175947 3.48490665 3.48490665 2.23214368 3.48490665\n",
      " 3.48490665 3.48490665 3.48490665 3.48490665 3.07944154 3.48490665\n",
      " 3.48490665 3.07944154 3.48490665 3.48490665 1.87546874 3.48490665\n",
      " 1.13353139 1.78015856 3.48490665 2.56861592 2.23214368 3.48490665\n",
      " 1.69314718 3.48490665 3.48490665 1.78015856 3.48490665 3.48490665\n",
      " 3.48490665 3.48490665 3.48490665 2.79175947 3.48490665 3.48490665\n",
      " 2.79175947 3.48490665 2.09861229 3.48490665 2.79175947 3.48490665\n",
      " 3.48490665]\n",
      "----------\n",
      "All words in the vocabulary ['00' '00000' '01' '02' '1999' '2020' 'adjustment' 'amount' 'customer'\n",
      " 'date' 'dbo' 'description' 'discription' 'docdate' 'docnumber' 'document'\n",
      " 'exec' 'fetching' 'fiscalyear' 'get' 'gettaxdetailsbydocnumber'\n",
      " 'gettaxstartingfromdate' 'id' 'input' 'join' 'master' 'net' 'nettotaldue'\n",
      " 'number' 'output' 'posted' 'quarter' 'quarters' 'query' 'sales' 'saved'\n",
      " 'select' 'settlement' 'settlements' 'show' 'shows' 'status' 'tax' 'taxid'\n",
      " 'taxmaster' 'taxsettlement' 'taxsettlementid' 'taxsettlementitems' 'till'\n",
      " 'tm' 'todate' 'total' 'totalinputvat' 'totaloutputvat' 'ts' 'tsi' 'using'\n",
      " 'value' 'vat' 'vatamount' 'year']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.36243055 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.29665301 ... 0.         0.         0.        ]]\n",
      "----------\n",
      "Tfidf representation for prompt:\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.55226453 0.         0.         0.         0.\n",
      "  0.         0.         0.62498031 0.46065348 0.         0.\n",
      "  0.30364763 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n",
      "1 :\n",
      "Euclidean Distance: (1, 1.4142135623730951)\n",
      "2 :\n",
      "Euclidean Distance: (2, 1.4142135623730951)\n",
      "3 :\n",
      "Euclidean Distance: (3, 1.4142135623730951)\n",
      "4 :\n",
      "Euclidean Distance: (4, 1.4142135623730951)\n",
      "5 :\n",
      "Euclidean Distance: (5, 1.4142135623730951)\n",
      "6 :\n",
      "Euclidean Distance: (6, 1.4142135623730951)\n",
      "7 :\n",
      "Euclidean Distance: (7, 1.4142135623730951)\n",
      "8 :\n",
      "Euclidean Distance: (8, 1.4142135623730951)\n",
      "9 :\n",
      "Euclidean Distance: (9, 1.4142135623730951)\n",
      "10 :\n",
      "Euclidean Distance: (10, 1.4142135623730951)\n",
      "11 :\n",
      "Euclidean Distance: (11, 1.318556328824465)\n",
      "12 :\n",
      "Euclidean Distance: (12, 1.3659963279047322)\n",
      "13 :\n",
      "Euclidean Distance: (13, 1.1456124741432907)\n",
      "14 :\n",
      "Euclidean Distance: (14, 1.3846378088904316)\n",
      "15 :\n",
      "Euclidean Distance: (15, 1.36282036381184)\n",
      "16 :\n",
      "Euclidean Distance: (16, 1.4142135623730951)\n",
      "17 :\n",
      "Euclidean Distance: (17, 1.3701880718753285)\n",
      "18 :\n",
      "Euclidean Distance: (18, 1.3736071617261905)\n",
      "19 :\n",
      "Euclidean Distance: (19, 1.3701880718753285)\n",
      "20 :\n",
      "Euclidean Distance: (20, 1.348654975454626)\n",
      "21 :\n",
      "Euclidean Distance: (21, 1.2650939610798169)\n",
      "22 :\n",
      "Euclidean Distance: (22, 1.2934352588457854)\n",
      "[(13, 1.1456124741432907), (21, 1.2650939610798169), (22, 1.2934352588457854), (11, 1.318556328824465), (20, 1.348654975454626), (15, 1.36282036381184), (12, 1.3659963279047322), (17, 1.3701880718753285), (19, 1.3701880718753285), (18, 1.3736071617261905), (14, 1.3846378088904316), (1, 1.4142135623730951), (2, 1.4142135623730951), (3, 1.4142135623730951), (4, 1.4142135623730951), (5, 1.4142135623730951), (6, 1.4142135623730951), (7, 1.4142135623730951), (8, 1.4142135623730951), (9, 1.4142135623730951), (10, 1.4142135623730951), (16, 1.4142135623730951)]\n",
      "check\n",
      "       Distance\n",
      "Index          \n",
      "13     1.145612\n",
      "21     1.265094\n",
      "22     1.293435\n",
      "After update\n",
      "[13, 21, 22]\n",
      "select * from TaxSettlement where quarter = 0 /* Shows all the tax settlement using quarters */\n",
      "EXEC GetTaxStartingFromDate '1999-01-02' /*show Tax Settlement from date */\n",
      "select * from TaxSettlement where ToDate <= '1999-01-02' /*show Tax Settlement only till date or until date only */\n",
      "       Distance\n",
      "Index          \n",
      "13     1.145612\n",
      "21     1.265094\n",
      "22     1.293435\n",
      "[[1.         0.21821789 0.18898224 0.16903085 0.16903085 0.15430335\n",
      "  0.15430335 0.15430335 0.14285714 0.16903085 0.15430335 0.40089186\n",
      "  0.19518001 0.43643578 0.21262163 0.30304576 0.         0.20203051\n",
      "  0.18898224 0.17817416 0.14824986 0.35856858 0.24174689]\n",
      " [0.21821789 1.         0.4330127  0.64549722 0.64549722 0.58925565\n",
      "  0.58925565 0.58925565 0.54554473 0.51639778 0.58925565 0.51031036\n",
      "  0.2981424  0.41666667 0.19487094 0.38575837 0.12909944 0.15430335\n",
      "  0.21650635 0.27216553 0.1132277  0.09128709 0.12309149]\n",
      " [0.18898224 0.4330127  1.         0.67082039 0.67082039 0.61237244\n",
      "  0.61237244 0.61237244 0.56694671 0.4472136  0.61237244 0.35355339\n",
      "  0.25819889 0.28867513 0.16876319 0.26726124 0.2236068  0.26726124\n",
      "  0.25       0.23570226 0.19611614 0.15811388 0.21320072]\n",
      " [0.16903085 0.64549722 0.67082039 1.         0.8        0.91287093\n",
      "  0.91287093 0.73029674 0.84515425 0.8        0.73029674 0.31622777\n",
      "  0.23094011 0.25819889 0.2515773  0.23904572 0.2        0.23904572\n",
      "  0.2236068  0.21081851 0.1754116  0.14142136 0.19069252]\n",
      " [0.16903085 0.64549722 0.67082039 0.8        1.         0.73029674\n",
      "  0.91287093 0.91287093 0.84515425 0.6        0.91287093 0.31622777\n",
      "  0.34641016 0.25819889 0.20126184 0.23904572 0.2        0.23904572\n",
      "  0.2236068  0.21081851 0.1754116  0.14142136 0.19069252]\n",
      " [0.15430335 0.58925565 0.61237244 0.91287093 0.73029674 1.\n",
      "  0.83333333 0.83333333 0.9258201  0.73029674 0.66666667 0.28867513\n",
      "  0.21081851 0.23570226 0.22965761 0.21821789 0.18257419 0.21821789\n",
      "  0.20412415 0.19245009 0.16012815 0.12909944 0.17407766]\n",
      " [0.15430335 0.58925565 0.61237244 0.91287093 0.91287093 0.83333333\n",
      "  1.         0.83333333 0.9258201  0.73029674 0.83333333 0.28867513\n",
      "  0.31622777 0.23570226 0.27558913 0.21821789 0.18257419 0.21821789\n",
      "  0.20412415 0.19245009 0.16012815 0.12909944 0.17407766]\n",
      " [0.15430335 0.58925565 0.61237244 0.73029674 0.91287093 0.83333333\n",
      "  0.83333333 1.         0.9258201  0.54772256 0.83333333 0.28867513\n",
      "  0.31622777 0.23570226 0.18372608 0.21821789 0.18257419 0.21821789\n",
      "  0.20412415 0.19245009 0.16012815 0.12909944 0.17407766]\n",
      " [0.14285714 0.54554473 0.56694671 0.84515425 0.84515425 0.9258201\n",
      "  0.9258201  0.9258201  1.         0.6761234  0.77151675 0.26726124\n",
      "  0.29277002 0.21821789 0.25514595 0.20203051 0.16903085 0.20203051\n",
      "  0.18898224 0.17817416 0.14824986 0.11952286 0.16116459]\n",
      " [0.16903085 0.51639778 0.4472136  0.8        0.6        0.73029674\n",
      "  0.73029674 0.54772256 0.6761234  1.         0.73029674 0.31622777\n",
      "  0.34641016 0.38729833 0.30189276 0.35856858 0.         0.35856858\n",
      "  0.3354102  0.31622777 0.26311741 0.14142136 0.28603878]\n",
      " [0.15430335 0.58925565 0.61237244 0.73029674 0.91287093 0.66666667\n",
      "  0.83333333 0.83333333 0.77151675 0.73029674 1.         0.28867513\n",
      "  0.42163702 0.35355339 0.22965761 0.32732684 0.18257419 0.32732684\n",
      "  0.30618622 0.28867513 0.24019223 0.12909944 0.26111648]\n",
      " [0.40089186 0.51031036 0.35355339 0.31622777 0.31622777 0.28867513\n",
      "  0.28867513 0.28867513 0.26726124 0.31622777 0.28867513 1.\n",
      "  0.63900965 0.81649658 0.39777864 0.75592895 0.         0.47245559\n",
      "  0.53033009 0.58333333 0.2773501  0.3354102  0.37688918]\n",
      " [0.19518001 0.2981424  0.25819889 0.23094011 0.34641016 0.21081851\n",
      "  0.31622777 0.31622777 0.29277002 0.34641016 0.42163702 0.63900965\n",
      "  1.         0.67082039 0.55194325 0.55205245 0.23094011 0.48304589\n",
      "  0.51639778 0.54772256 0.30382181 0.24494897 0.44038551]\n",
      " [0.43643578 0.41666667 0.28867513 0.25819889 0.25819889 0.23570226\n",
      "  0.23570226 0.23570226 0.21821789 0.38729833 0.35355339 0.81649658\n",
      "  0.67082039 1.         0.38974188 0.69436507 0.         0.54006172\n",
      "  0.57735027 0.61237244 0.33968311 0.27386128 0.36927447]\n",
      " [0.21262163 0.19487094 0.16876319 0.2515773  0.20126184 0.22965761\n",
      "  0.27558913 0.18372608 0.25514595 0.30189276 0.22965761 0.39777864\n",
      "  0.55194325 0.38974188 1.         0.33076163 0.2515773  0.27062315\n",
      "  0.28127198 0.29170434 0.1985831  0.17789202 0.21588259]\n",
      " [0.30304576 0.38575837 0.26726124 0.23904572 0.23904572 0.21821789\n",
      "  0.21821789 0.21821789 0.20203051 0.35856858 0.32732684 0.75592895\n",
      "  0.55205245 0.69436507 0.33076163 1.         0.         0.42857143\n",
      "  0.46770717 0.50395263 0.26207121 0.25354628 0.34188173]\n",
      " [0.         0.12909944 0.2236068  0.2        0.2        0.18257419\n",
      "  0.18257419 0.18257419 0.16903085 0.         0.18257419 0.\n",
      "  0.23094011 0.         0.2515773  0.         1.         0.\n",
      "  0.         0.         0.         0.14142136 0.        ]\n",
      " [0.20203051 0.15430335 0.26726124 0.23904572 0.23904572 0.21821789\n",
      "  0.21821789 0.21821789 0.20203051 0.35856858 0.32732684 0.47245559\n",
      "  0.48304589 0.54006172 0.27062315 0.42857143 0.         1.\n",
      "  0.80178373 0.75592895 0.36689969 0.25354628 0.34188173]\n",
      " [0.18898224 0.21650635 0.25       0.2236068  0.2236068  0.20412415\n",
      "  0.20412415 0.20412415 0.18898224 0.3354102  0.30618622 0.53033009\n",
      "  0.51639778 0.57735027 0.28127198 0.46770717 0.         0.80178373\n",
      "  1.         0.82495791 0.34320324 0.23717082 0.31980107]\n",
      " [0.17817416 0.27216553 0.23570226 0.21081851 0.21081851 0.19245009\n",
      "  0.19245009 0.19245009 0.17817416 0.31622777 0.28867513 0.58333333\n",
      "  0.54772256 0.61237244 0.29170434 0.50395263 0.         0.75592895\n",
      "  0.82495791 1.         0.32357511 0.2236068  0.30151134]\n",
      " [0.14824986 0.1132277  0.19611614 0.1754116  0.1754116  0.16012815\n",
      "  0.16012815 0.16012815 0.14824986 0.26311741 0.24019223 0.2773501\n",
      "  0.30382181 0.33968311 0.1985831  0.26207121 0.         0.36689969\n",
      "  0.34320324 0.32357511 1.         0.12403473 0.3344968 ]\n",
      " [0.35856858 0.09128709 0.15811388 0.14142136 0.14142136 0.12909944\n",
      "  0.12909944 0.12909944 0.11952286 0.14142136 0.12909944 0.3354102\n",
      "  0.24494897 0.27386128 0.17789202 0.25354628 0.14142136 0.25354628\n",
      "  0.23717082 0.2236068  0.12403473 1.         0.60677988]\n",
      " [0.24174689 0.12309149 0.21320072 0.19069252 0.19069252 0.17407766\n",
      "  0.17407766 0.17407766 0.16116459 0.28603878 0.26111648 0.37688918\n",
      "  0.44038551 0.36927447 0.21588259 0.34188173 0.         0.34188173\n",
      "  0.31980107 0.30151134 0.3344968  0.60677988 1.        ]]\n",
      "Second maximum similarity: 0.4364357804719848\n",
      "Second maximum similarity: 0.4364357804719848\n",
      "0.2182178902359924\n",
      "Show me all tax settlements from Quarter 1\n",
      "0.9999999999999998\n",
      "select * from TaxSettlement /* Shows all the tax settlement */\n",
      "0.40089186286863654\n",
      "select * from TaxSettlement where quarter = 0 /* Shows all the tax settlement using quarters */\n",
      "0.4364357804719848\n",
      "select * from TaxSettlement where FiscalYear = '2020' /* Shows all the tax settlement for that year*/\n",
      "0.3030457633656632\n",
      "EXEC GetTaxStartingFromDate '1999-01-02' /*show Tax Settlement from date */\n",
      "0.3585685828003181\n",
      "select * from TaxSettlement where ToDate <= '1999-01-02' /*show Tax Settlement only till date or until date only */\n",
      "0.24174688920761406\n",
      "6\n",
      "[(0, 0.9999999999999998), (11, 0.40089186286863654), (13, 0.4364357804719848), (15, 0.3030457633656632), (21, 0.3585685828003181), (22, 0.24174688920761406)]\n",
      "0\n",
      "check\n",
      "0.4364357804719848\n",
      "After update\n",
      "[0, 13, 11, 21]\n",
      "Show me all tax settlements from Quarter 1\n",
      "select * from TaxSettlement where quarter = 0 /* Shows all the tax settlement using quarters */\n",
      "EXEC GetTaxStartingFromDate '1999-01-02' /*show Tax Settlement from date */\n",
      "tensor([[-0.1226, -0.0345, -1.1126,  ..., -0.1646, -1.7877, -1.1997],\n",
      "        [-0.6562, -0.0572, -0.8480,  ..., -0.4381, -1.2822, -0.8279],\n",
      "        [-1.0432, -0.0860, -1.3126,  ...,  0.4540, -1.2264, -2.1156],\n",
      "        [-0.7314, -0.1469, -0.6913,  ...,  0.1098, -1.1863, -2.1390]])\n",
      "Sentence embedding:\n",
      "0.8427689075469971\n",
      "Sentence Embeding after adding Cosine weight and tfidf\n",
      "0.28025501016612275\n",
      "Max\n",
      "0.28025501016612275\n",
      "Sentence embedding:\n",
      "0.7553112506866455\n",
      "Sentence Embeding after adding Cosine weight and tfidf\n",
      "0.1692203633325606\n",
      "Sentence embedding:\n",
      "0.768722653388977\n",
      "Sentence Embeding after adding Cosine weight and tfidf\n",
      "0.18265005395734604\n",
      "++++++++++++++++++++++++++++++++\n",
      "select * from TaxSettlement where quarter = 0 /* Shows all the tax settlement using quarters */\n",
      "------------------------------------------\n",
      "The query contains the literal '\\d'.\n",
      "Value: 1\n",
      "Updated query: select * from TaxSettlement where quarter = 1 /* Shows all the tax settlement using quarters */\n",
      "Are you satisfied\n",
      " Shows all the tax settlement using quarters \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type 'yes' or 'no':  yes\n"
     ]
    }
   ],
   "source": [
    "Reinforcement_Data =[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "Reinforcement_Data2d= [0,0,0,0]\n",
    "data = ['select * from dbo.Sales /* This query shows all the sales */', 'select * from dbo.Customer', 'select Id from dbo.Sales','select DocNumber from dbo.Sales','select Id, DocDate from dbo.Sales','select Id, DocNumber from dbo.Sales', 'select DocDate, DocNumber from dbo.Sales','select Id, DocDate, DocNumber from dbo.Sales', 'Select * from Sales where Id = \\\\d', \"Select * from dbo.Sales where DocNumber = '\\\\d'\", \"select * from TaxSettlement /* Shows all the tax settlement */\", \"select * from TaxSettlement where DocNumber = 'TS-00000' /* Shows only the tax settlement using Document Number */\", \"select * from TaxSettlement where quarter = 0 /* Shows all the tax settlement using quarters */\", \"select TaxId, Description, Amount, Adjustment, VATAmount from TaxMaster tm join TaxSettlementItems tsi on tm.Id=tsi.TaxId join TaxSettlement Ts on Ts.Id = tsi.TaxSettlementId where Ts.DocNumber = 'TS-00000' /* Shows all the tax master or description fetching discription using document NUMBER from tax settlement*/\", \"select * from TaxSettlement where FiscalYear = '2020' /* Shows all the tax settlement for that year*/\",\"EXEC dbo.GetTaxDetailsByDocNumber 'Ts-00000' \", \"select * from TaxSettlement where TotalInputVat = 0.00 /*using Tax Settlement to get Total Input vat */\", \"select * from TaxSettlement where TotalOutputVat = 0.00 /*using Total Output vat value to get the tax settlement*/ \", \"select * from TaxSettlement where NetTotalDue = 0.00 /* using the total net vat to get the tax settlement*/\", \"select * from TaxSettlement where Status = 'saved or posted' /* using status posted or saved to show TaxSettlement */\", \"EXEC GetTaxStartingFromDate '1999-01-02' /*show Tax Settlement from date */\",\"select * from TaxSettlement where ToDate <= '1999-01-02' /*show Tax Settlement only till date or until date only */\"]\n",
    "data2d =[\"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and ToDate <= 'some_end_date' /* This query retrieves data with both a starting date (FromDate) and an ending date (ToDate), covering a range from the start to end date [1][2] */\", \"select * from dbo.TaxSettlement where FromDate >= 'some_start_date' and DocNumber= 'some_doc_number'; /*Filter TaxSettlement records with a starting date (FromDate >= some_start_date) and a specific document number (DocNumber = some_doc_number that contains TS or TS- or TS-00) [1][3]*/\", \"SELECT * FROM dbo.TaxSettlement WHERE ToDate <= 'some_end_date' AND DocNumber = 'some_doc_number' /* Filter TaxSettlement records with an end date (ToDate <= some_end_date) and a specific document number (DocNumber = some_doc_number  that contains TS or TS- or TS-00) [2][3] */ \", \"select * from dbo.TaxSettlement where TotalInputVat = 'input_value' and TotalOutputVat = 'output_value' /* Query for TaxSettlement records where TotalInputVat is set to a specified value [4] and TotalOutputVat is set to another specified value [5] */\"]\n",
    "\n",
    "\n",
    "# prompt = \"display tax settlement on Quarter-1\"\n",
    "prompt = input(\"Enter your Query\")\n",
    "and_count = len(re.findall(r'\\band\\b', prompt))\n",
    "\n",
    "\n",
    "if and_count == 0:\n",
    "    document= data\n",
    "    document.insert(0, prompt)\n",
    "else:\n",
    "    document = data2d\n",
    "    # document.append(data)\n",
    "    document.insert(0, prompt)\n",
    "\n",
    "\n",
    "print(stopwords.words(\"english\"))\n",
    "# processed_docs2 = [doc.lower().replace(stopwords.words(\"english\"),\"\") for doc in document]\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "processed_docs2 = [' '.join([word for word in doc.lower().split() if word not in stop_words]) for doc in document]\n",
    "# processed_docs2 = [doc.lower().replace(stopwords.words(\"english\"), '') for doc in document]\n",
    "processed_docs2\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "vectorized = tfidf.fit_transform(processed_docs2)\n",
    "\n",
    "\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names_out())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",vectorized.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([prompt])\n",
    "print(\"Tfidf representation for prompt:\\n\", temp.toarray())\n",
    "\n",
    "mat = vectorized.toarray()\n",
    "count =1\n",
    "distance= []\n",
    "\n",
    "while len(processed_docs2) > count:\n",
    "    print(count, \":\")\n",
    "    distance.append((count ,euclidean(mat[0], mat[count])))\n",
    "    print(\"Euclidean Distance:\", distance[count-1])\n",
    "    count+=1;\n",
    "\n",
    "\n",
    "\n",
    "distance.sort(key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# Get the top 3 highest similarity scores\n",
    "\n",
    "top_3_index_tf = distance[:3]\n",
    "print(distance)\n",
    "count=3\n",
    "while( count<len(distance) and top_3_index_tf[2][1] == distance[count][1]):\n",
    "    top_3_index_tf.append(distance[count])\n",
    "    count+=1\n",
    "      \n",
    "index_tfidf=[]\n",
    "print(\"check\")\n",
    "df_tf = pd.DataFrame(top_3_index_tf, columns=['Index', 'Distance'])\n",
    "\n",
    "# Set the 'Index' column as the DataFrame's index\n",
    "df_tf.set_index('Index', inplace=True)\n",
    "print(df_tf)\n",
    "for item in top_3_index_tf:\n",
    "    index_tfidf.append(item[0])  # item[0] is the index from the tuple\n",
    "\n",
    "# Display the updated index (which now contains only the index values)\n",
    "print(\"After update\")\n",
    "print(index_tfidf)\n",
    "\n",
    "\n",
    "sentences_tfidf = []\n",
    "count=0;\n",
    "while count<3:\n",
    "    sentences_tfidf.append(document[index_tfidf[count]])\n",
    "    print(document[index_tfidf[count]])\n",
    "    count+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df_tf)\n",
    "# distance_at_index = df_tf.loc[17, 'Distance']\n",
    "# print(distance_at_index)\n",
    "\n",
    "\n",
    "processed_docs = [doc.lower()for doc in document]\n",
    "processed_docs\n",
    "\n",
    "\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names_out(), \n",
    "                  index=processed_docs)\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "similarity=cosine_similarity(df, df)\n",
    "print(similarity)\n",
    "\n",
    "index= []\n",
    "count = 0\n",
    "sentences=[]\n",
    "first_row = similarity[0]\n",
    "\n",
    "# Convert it to a list and remove the maximum value (which is 1.0, the self-similarity)\n",
    "sorted_row = sorted(first_row, reverse=True)\n",
    "\n",
    "# The second value in the sorted list is the second maximum\n",
    "second_max = sorted_row[1]\n",
    "\n",
    "print(\"Second maximum similarity:\", second_max)\n",
    "\n",
    "\n",
    "# maxim= second_max - (second_max/10)\n",
    "# print(maxim)\n",
    "# while(count < len(processed_docs)):\n",
    "#     if(similarity[0][count] > maxim):\n",
    "#         index.append(count)\n",
    "#     count = count+1\n",
    "\n",
    "# print(len(index))\n",
    "# print(index[1])\n",
    "\n",
    "print(\"Second maximum similarity:\", second_max)\n",
    "\n",
    "\n",
    "maxim= second_max - (second_max/2)\n",
    "print(maxim)\n",
    "while(count < len(processed_docs)):\n",
    "    if(similarity[0][count] > maxim):\n",
    "        # sentences.append(document[count])\n",
    "        print(document[count])\n",
    "        print(similarity[0][count])\n",
    "        index.append((count, similarity[0][count]))\n",
    "    count = count+1\n",
    "\n",
    "print(len(index))\n",
    "print(index)\n",
    "\n",
    "print(index[0][0])\n",
    "\n",
    "index.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 3 highest similarity scores\n",
    "top_3_index = index[:4]\n",
    "\n",
    "new_data_df = pd.DataFrame(top_3_index, columns=['Index', 'Cosine'])\n",
    "new_data_df.set_index('Index', inplace=True)\n",
    "\n",
    "\n",
    "index=[]\n",
    "print(\"check\")\n",
    "print(top_3_index[1][1])\n",
    "\n",
    "for item in top_3_index:\n",
    "    index.append(item[0])  # item[0] is the index from the tuple\n",
    "\n",
    "# Display the updated index (which now contains only the index values)\n",
    "print(\"After update\")\n",
    "print(index)\n",
    "\n",
    "count=0;\n",
    "while count<4 and count<len(index):\n",
    "    if count ==0 or index[count] in index_tfidf:\n",
    "        sentences.append(document[index[count]])\n",
    "        print(document[index[count]])\n",
    "    count+=1\n",
    "\n",
    "\n",
    "new_data_df\n",
    "\n",
    "df_tf['Cosine'] = df_tf.index.map(new_data_df['Cosine'])\n",
    "\n",
    "\n",
    "df_tf\n",
    "\n",
    "average_cosine = df_tf['Cosine'].mean()\n",
    "df_tf['Cosine'].fillna(0, inplace=True)\n",
    "# Replace NaN values in the 'Cosine' column with the calculated average\n",
    "df_tf['Cosine'].fillna(average_cosine, inplace=True)\n",
    "df_tf['Cosine'] = df_tf['Cosine'].replace(0, average_cosine)\n",
    "\n",
    "df_tf\n",
    "\n",
    "\n",
    "index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "index2= []\n",
    "for ind in index:\n",
    "    if ind in df_tf.index:\n",
    "        index2.append(ind)\n",
    "\n",
    "\n",
    "index = index2\n",
    "index.insert(0,0)\n",
    "nan_indices = df_tf[df_tf['Cosine'] == average_cosine].index.tolist()\n",
    "index.extend(nan_indices)\n",
    "\n",
    "count =0;\n",
    "while count < len(nan_indices):\n",
    "    sentences.append(document[nan_indices[count]])\n",
    "    count += 1\n",
    "\n",
    "Return = document[index[1]]\n",
    "\n",
    "\n",
    "#using tfidf vectors also now\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "NotSatisfiedList = []\n",
    "# Sentences we want sentence embeddings for\n",
    "# Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings)\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "max = 0;\n",
    "counter=1;\n",
    "value=0;\n",
    "Possible_Sentence=\"check\"\n",
    "while(len(sentences) > counter):\n",
    "    print(\"Sentence embedding:\")\n",
    "    print( (sentence_embeddings[0] @ sentence_embeddings[counter]).item() )\n",
    "    print(\"Sentence Embeding after adding Cosine weight and tfidf\")\n",
    "    val = ((sentence_embeddings[0] @ sentence_embeddings[counter]).item() * df_tf.loc[index[counter],'Cosine'])/ (df_tf.loc[index[counter],'Distance'] ** 2)\n",
    "    print( val )\n",
    "    if(val >= max):\n",
    "        max=val\n",
    "        value=counter;\n",
    "        print(\"Max\")\n",
    "        print(max)\n",
    "        Possible_Sentence = sentences[counter];\n",
    "\n",
    "    NotSatisfiedList.append((sentences[counter], val)) \n",
    "    counter = counter +1\n",
    "\n",
    "print(\"++++++++++++++++++++++++++++++++\")\n",
    "print(Possible_Sentence)\n",
    "\n",
    "NotSatisfiedList\n",
    "NotSatisfiedList.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def FetchDataIntoSql(Possible_Sentence):\n",
    "    \n",
    "    \n",
    "    if(and_count ==0):\n",
    "        # Example query\n",
    "        query = Possible_Sentence\n",
    "        \n",
    "        # Check if the query contains the literal '\\d'\n",
    "        pattern = r\"\\d|'TS-\\d+'|'posted'|'saved or posted'\"  # Use double backslashes to escape the backslash\n",
    "        \n",
    "        if re.search(pattern, query):\n",
    "            print(\"The query contains the literal '\\\\d'.\")\n",
    "                \n",
    "            if re.search(r\"TS-\\d\",query):\n",
    "                    print(\"DocNumber\")\n",
    "                    replacement_invoice= re.search(r\"TS-\\d+\",prompt, re.IGNORECASE)\n",
    "                    if replacement_invoice:\n",
    "                        print(\"DocNumber:\",replacement_invoice.group() )  # This will print \"TS-12345\"\n",
    "                        replacement_invoice = replacement_invoice.group()\n",
    "                        updated_query = re.sub(r\"TS-\\d+\", replacement_invoice, query)\n",
    "                        print(\"Updated query:\", updated_query)\n",
    "                \n",
    "            elif re.search(r\"saved|posted\", query):\n",
    "                replacement_number = re.search(r\"saved|posted\", prompt)\n",
    "                if replacement_number:\n",
    "                        replacement_number_text = replacement_number.group()  # Extract the matched text as a string\n",
    "                        print(\"Value:\", replacement_number_text)\n",
    "                        updated_query = re.sub(r\"saved or posted\", replacement_number_text, query)\n",
    "                        print(\"Updated query:\", updated_query)\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                replacement_number = re.search(r\"\\d{4}-\\d{2}-\\d{2}|\\d+.\\d+|\\d+\", prompt)\n",
    "                if replacement_number:\n",
    "                        replacement_number_text = replacement_number.group()  # Extract the matched text as a string\n",
    "                        print(\"Value:\", replacement_number_text)\n",
    "                        updated_query = re.sub(r\"\\d{4}-\\d{2}-\\d{2}|\\d+.\\d+|\\d+\", replacement_number_text, query)\n",
    "                        print(\"Updated query:\", updated_query)\n",
    "        else:\n",
    "            print(\"The query does not contain the literal '\\\\d'.\")\n",
    "            updated_query = query\n",
    "            return updated_query\n",
    "    \n",
    "    else:\n",
    "        values = re.findall(r'\\[(\\d+)\\]', Possible_Sentence)\n",
    "    \n",
    "        # Convert each matched value to an integer\n",
    "        values = [int(value) for value in values]\n",
    "        print(values)\n",
    "        \n",
    "        questions = ['what is the year', ['what is the starting date','some_start_date'], ['what is the ending date','some_end_date'],['what is the document number','some_doc_number'] , ['what is the total input vat','input_value'], ['what is the total output vat', 'output_value'],'what is the total net vat']\n",
    "    \n",
    "        print(\"multiple value\")\n",
    "        for val in values:\n",
    "            print(val)\n",
    "            QA_input = {\n",
    "                'question': questions[val][0],\n",
    "                'context': prompt\n",
    "            }\n",
    "            res = nlp2(QA_input)\n",
    "            print(res)\n",
    "            print(res['answer'])\n",
    "            answer = res['answer']\n",
    "            \n",
    "            print(questions[val][1])\n",
    "            updated_query = Possible_Sentence.replace(questions[val][1], answer)\n",
    "            Possible_Sentence= updated_query\n",
    "        print(updated_query)\n",
    "        return updated_query\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "FetchDataIntoSql(Possible_Sentence)\n",
    "\n",
    "\n",
    "ans = \"no\"\n",
    "count =1\n",
    "print(\"Are you satisfied\")\n",
    "while(ans == 'no' and count <= 3):\n",
    "    match = re.search(r'/\\*(.*?)\\*/', Possible_Sentence)\n",
    "    \n",
    "    if match:\n",
    "        print(match.group(1))  # This will print the text between /* and */\n",
    "        while True:\n",
    "            ans = input(\"Please type 'yes' or 'no': \").lower()\n",
    "            if ans in ['yes', 'no']:\n",
    "                break  # Exit the loop if valid input is entered\n",
    "            else:\n",
    "                print(\"Invalid input. Please type 'yes' or 'no'.\")\n",
    "        if(ans == 'no' and count < 3):\n",
    "            Possible_Sentence = str(NotSatisfiedList[count][0])\n",
    "            count +=1\n",
    "\n",
    "    else:\n",
    "        print(\"No match found\")\n",
    "        break;\n",
    "\n",
    "\n",
    "if ans == 'yes' and count > 1:\n",
    "    FetchDataIntoSql(Possible_Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e09fb-79b2-448d-9720-65749cca54e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a78ab-c97e-4ca1-8248-1c4891067d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adf936-e2a9-491c-91a1-a7f14e0e2ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d054dff-b15f-476a-9c97-f8ca370da915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4658698-1dd8-418d-bbe9-115ffb435b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330ebcc-ca0a-4afd-a00c-3dee4166e9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c308b33-6691-4d92-9e97-c73cd91f5fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2114935-daf9-48ca-bef1-03877fbd7f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeeb710-b6a3-4380-a95a-f2a72abb8fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7d583-e682-4c86-990c-a53686a89f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08401c24-2352-4840-b250-55e5fe1ba984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31ff8ec7",
   "metadata": {},
   "source": [
    "<h1>IGNORE THE BELOW FOR NOW<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1883,
   "id": "fb9f0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "values = re.findall(r'\\[(\\d+)\\]', Possible_Sentence)\n",
    "\n",
    "# Convert each matched value to an integer\n",
    "values = [int(value) for value in values]\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1473,
   "id": "b5144f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['what is the year', ['what is the starting date','some_start_date'], ['what is the ending date','some_end_date'],['what is the document number','some_doc_number'] ,'what is the total net vat', 'what is the total output vat', 'what is the total input vat']\n",
    "\n",
    "for val in values:\n",
    "    print(val)\n",
    "    QA_input = {\n",
    "        'question': questions[val][0],\n",
    "        'context': prompt\n",
    "    }\n",
    "    res = nlp2(QA_input)\n",
    "    print(res)\n",
    "    print(res['answer'])\n",
    "    answer = res['answer']\n",
    "\n",
    "    \n",
    "    \n",
    "    updated_query = Possible_Sentence.replace(questions[val][1], answer)\n",
    "    Possible_Sentence= updated_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "id": "547f693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.4917096495628357, 'start': 12, 'end': 20, 'answer': 'TS-00001'}\n",
      "TS-00001\n"
     ]
    }
   ],
   "source": [
    "questions = ['what is the year', ['what is the starting date','some_start_date'], ['what is the ending date','some_end_date'] ,'what is the total net vat', 'what is the total output vat', 'what is the total input vat']\n",
    "QA_input = {\n",
    "    'question': questions[2][0],\n",
    "    'context': prompt\n",
    "}\n",
    "res = nlp2(QA_input)\n",
    "print(res)\n",
    "print(res['answer'])\n",
    "answer = res['answer']\n",
    "\n",
    "\n",
    "updated_query = Possible_Sentence.replace(questions[1][1], answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "id": "55a6e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_sentence = re.sub(r'\\bDoc', '', questions[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1476,
   "id": "4c2af48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXEC dbo.GetTaxDetailsByDocNumber 'Ts-00000' \n"
     ]
    }
   ],
   "source": [
    "print(updated_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1800,
   "id": "74e409a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"INV-\\d+\" # Use double backslashes to escape the backslash\n",
    "\n",
    "if re.search(pattern, prompt):\n",
    "    print(\"The query contains the literal 'INV-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb885d9",
   "metadata": {},
   "source": [
    "<h2>Getting the Connection set<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "42cd4559",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=5181): Max retries exceeded with url: /api/Sales?text=EXEC+GetTaxStartingFromDate+%272024%27+%2F%2Ashow+Tax+Settlement+from+date+%2A%2F (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002CBC18CB700>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\http\\client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000002CBC18CB700>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5181): Max retries exceeded with url: /api/Sales?text=EXEC+GetTaxStartingFromDate+%272024%27+%2F%2Ashow+Tax+Settlement+from+date+%2A%2F (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002CBC18CB700>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[277], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:5181/api/Sales\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: updated_query\n\u001b[0;32m      4\u001b[0m }\n\u001b[1;32m----> 5\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mstatus_code)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5181): Max retries exceeded with url: /api/Sales?text=EXEC+GetTaxStartingFromDate+%272024%27+%2F%2Ashow+Tax+Settlement+from+date+%2A%2F (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002CBC18CB700>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"http://localhost:5181/api/Sales\"\n",
    "params = {\n",
    "    \"text\": updated_query\n",
    "}\n",
    "r = requests.post(url, params=params)\n",
    "\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "01718b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TaxId': 22,\n",
       "  'Description': 'VAT on Domestic Sales',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 24,\n",
       "  'Description': 'VAT on International Sales',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 26,\n",
       "  'Description': 'VAT on Zero Rated Sales',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 28,\n",
       "  'Description': 'VAT on GCC Sales',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 30,\n",
       "  'Description': 'VAT on Exempted Sales',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 21,\n",
       "  'Description': 'VAT on Domestic Purchases',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 23,\n",
       "  'Description': 'VAT on International Purchases',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 25,\n",
       "  'Description': 'VAT on Zero Rated Purchases',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 27,\n",
       "  'Description': 'VAT on GCC Purchases',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0},\n",
       " {'TaxId': 29,\n",
       "  'Description': 'VAT on Exempted Purchases',\n",
       "  'Amount': 0.0,\n",
       "  'Adjustment': 0.0,\n",
       "  'VATAmount': 0.0}]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366b27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6b447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
